import re
import datetime
from typing import Optional, Tuple

import pandas as pd
import requests
import streamlit as st
from bs4 import BeautifulSoup


# ========= ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =========

def _safe_text(x) -> str:
    if x is None:
        return ""
    try:
        if pd.isna(x):
            return ""
    except Exception:
        pass
    s = str(x).strip()
    if s.lower() == "nan":
        return ""
    return s


def _to_int(text: str) -> Optional[int]:
    if text is None:
        return None
    t = str(text).strip()
    if t == "":
        return None
    t = t.replace(",", "")
    digits = "".join([c for c in t if c.isdigit()])
    return int(digits) if digits != "" else None


def _to_float_pct(pct_text: str) -> Optional[float]:
    if pct_text is None:
        return None
    s = str(pct_text).replace("ï¼…", "%")
    m = re.findall(r"[-+]?\d+(?:\.\d+)?\s*%", s)
    if len(m) > 0:
        x = m[0].replace("%", "").strip()
        try:
            return float(x)
        except Exception:
            return None
    m2 = re.findall(r"[-+]?\d+(?:\.\d+)?", s)
    if len(m2) > 0:
        try:
            return float(m2[0])
        except Exception:
            return None
    return None


# ========= Yanoshin TDnet =========

def _normalize_company_code(s) -> str:
    s = "" if s is None else str(s).strip()
    if len(s) == 5 and s.endswith("0") and s[:4].isdigit():
        return s[:4]
    digits = "".join([c for c in s if c.isdigit()])
    return digits[:4] if len(digits) >= 4 else digits


def _extract_date_from_pubdate(pubdate: str) -> Optional[datetime.date]:
    s = _safe_text(pubdate)
    if s == "":
        return None

    m = re.search(r"(\d{4})[-/](\d{2})[-/](\d{2})", s)
    if m:
        try:
            return datetime.date(int(m.group(1)), int(m.group(2)), int(m.group(3)))
        except Exception:
            return None

    m2 = re.search(r"(\d{4})(\d{2})(\d{2})", s)
    if m2:
        try:
            return datetime.date(int(m2.group(1)), int(m2.group(2)), int(m2.group(3)))
        except Exception:
            return None

    return None


def attach_disclosures(df_in: pd.DataFrame, debug: bool = False) -> pd.DataFrame:
    url_recent = "https://webapi.yanoshin.jp/webapi/tdnet/list/recent.json2?limit=2000"


    def _fetch(url: str, source_tag: str) -> pd.DataFrame:
        r = requests.get(url, timeout=20)
        r.raise_for_status()
        data = r.json()

        items = data.get("items")
        if items is None:
            items = data.get("result")
        if items is None:
            items = data
        if not isinstance(items, list):
            items = []

        rows = []
        for it in items:
            code = _normalize_company_code(it.get("company_code"))
            code = _safe_text(code).zfill(4)

            title = _safe_text(it.get("title", ""))
            doc_url = _safe_text(it.get("document_url", ""))
            pubdate = _safe_text(it.get("pubdate", ""))

            rows.append(
                {
                    "code": code,
                    "source_tag": source_tag,
                    "title": title,
                    "document_url": doc_url,
                    "pubdate": pubdate,
                }
            )

        if len(rows) == 0:
            return pd.DataFrame(columns=["code", "source_tag", "title", "document_url", "pubdate"])
        return pd.DataFrame(rows)

    td = _fetch(url_recent, source_tag="recent")
    td_today = td  # è¨ºæ–­è¡¨ç¤ºäº’æ›ã®ãŸã‚ï¼ˆãªãã¦ã‚‚OKï¼‰
    td_yesterday = td

    if len(td) > 0:
        td["code"] = td["code"].apply(_safe_text).str.zfill(4)
        td["document_url"] = td["document_url"].apply(_safe_text)
        td["title"] = td["title"].apply(_safe_text)
        td["pubdate"] = td["pubdate"].apply(_safe_text)

        td = td[(td["code"] != "") & (td["document_url"] != "")].copy()
        td["pub_date_only"] = td["pubdate"].apply(_extract_date_from_pubdate)

        td = td.drop_duplicates(subset=["code", "document_url"], keep="first")

    # è¦³æ¸¬æ—¥ä»˜ãƒ™ãƒ¼ã‚¹ã§ã€Œå½“æ—¥/å‰æ—¥ã€ï¼ˆè¿”ã£ã¦ããŸä¸­ã§æœ€æ–°=å½“æ—¥ã€æ¬¡ç‚¹=å‰æ—¥ï¼‰
    max_date = None
    prev_date = None
    if len(td) > 0:
        dates = [d for d in td["pub_date_only"].tolist() if isinstance(d, datetime.date)]
        uniq = sorted(set(dates), reverse=True)
        if len(uniq) >= 1:
            max_date = uniq[0]
        if len(uniq) >= 2:
            prev_date = uniq[1]

    def _day_tag(d: Optional[datetime.date]) -> str:
        if d is None:
            return ""
        if max_date is not None and d == max_date:
            return "å½“æ—¥"
        if prev_date is not None and d == prev_date:
            return "å‰æ—¥"
        return ""

    if len(td) > 0:
        td["day_tag"] = td["pub_date_only"].apply(_day_tag)
    else:
        td["day_tag"] = ""

    if debug:
        st.write("ã€è¨ºæ–­ã€‘Yanoshinä»¶æ•° today:", int(len(td_today)))
        st.write("ã€è¨ºæ–­ã€‘Yanoshinä»¶æ•° yesterday:", int(len(td_yesterday)))
        st.write("ã€è¨ºæ–­ã€‘Yanoshinçµåˆå¾Œä»¶æ•°ï¼ˆé‡è¤‡é™¤å»å¾Œï¼‰:", int(len(td)))
        st.write("ã€è¨ºæ–­ã€‘pubdateã‚µãƒ³ãƒ—ãƒ«å…ˆé ­10:", td["pubdate"].dropna().head(10).tolist())
        st.write("ã€è¨ºæ–­ã€‘pub_date_onlyæ—¥ä»˜åˆ¥ä»¶æ•°:", td["pub_date_only"].value_counts(dropna=False).to_dict())
        st.write("ã€è¨ºæ–­ã€‘å–å¾—URL:", url_recent)

        if len(td) > 0:
            uniq_dates = sorted(
                set([d for d in td["pub_date_only"].tolist() if isinstance(d, datetime.date)]),
                reverse=True,
            )
            st.write("ã€è¨ºæ–­ã€‘pub_date_onlyãƒ¦ãƒ‹ãƒ¼ã‚¯ï¼ˆæ–°ã—ã„é †ï¼‰:", [str(x) for x in uniq_dates])
            st.write("ã€è¨ºæ–­ã€‘å½“æ—¥ã¨ã¿ãªã™æ—¥ä»˜:", str(max_date) if max_date else "ãªã—")
            st.write("ã€è¨ºæ–­ã€‘å‰æ—¥ã¨ã¿ãªã™æ—¥ä»˜:", str(prev_date) if prev_date else "ãªã—")
            st.write("ã€è¨ºæ–­ã€‘day_tagå†…è¨³:", td["day_tag"].value_counts(dropna=False).to_dict())

    def _rank(tag: str) -> int:
        if tag == "å½“æ—¥":
            return 0
        if tag == "å‰æ—¥":
            return 1
        return 9

    # â˜…ä¿®æ­£ï¼šå½“æ—¥/å‰æ—¥æ–‡å­—ã‚’æ¶ˆã—ã¦ğŸŸ¦ğŸŸ¨ã ã‘ã«ã™ã‚‹
    def _decorate_title(day_tag: str, title: str) -> str:
        title = _safe_text(title)
        if day_tag == "å½“æ—¥":
            prefix = "ğŸŸ¦ "
        elif day_tag == "å‰æ—¥":
            prefix = "ğŸŸ¨ "
        else:
            prefix = ""
        if title == "":
            return prefix + "(ã‚¿ã‚¤ãƒˆãƒ«ãªã—)"
        return prefix + title

    # code -> [(day_tag, title_text, url), ...]
    by_code = {}
    if len(td) > 0:
        td2 = td.copy()
        td2["rank"] = td2["day_tag"].apply(_rank)
        td2 = td2.sort_values(by=["code", "rank"], ascending=True)

        for _, row in td2.iterrows():
            c = _safe_text(row.get("code", "")).zfill(4)
            day_tag = _safe_text(row.get("day_tag", ""))
            title_text = _decorate_title(day_tag, row.get("title", ""))
            url = _safe_text(row.get("document_url", ""))
            by_code.setdefault(c, []).append((day_tag, title_text, url))

    df_out = df_in.copy()
    df_out["code"] = df_out["code"].astype(str).str.strip().str.zfill(4)

    df_out["é–‹ç¤ºä»¶æ•°"] = df_out["code"].apply(lambda c: len(by_code.get(c, [])))

    def _get_item(c, i):
        items = by_code.get(c, [])
        if i < len(items):
            _day_tag, title_text, url = items[i]
            return title_text, url
        return "", ""

    for i in range(3):
        df_out[f"é–‹ç¤ºã‚¿ã‚¤ãƒˆãƒ«{i+1}"] = df_out["code"].apply(lambda c, i=i: _get_item(c, i)[0])
        df_out[f"PDFãƒªãƒ³ã‚¯{i+1}"] = df_out["code"].apply(lambda c, i=i: _get_item(c, i)[1])

    # è©³ç´°ï¼ˆæœ€å¤§5ä»¶ï¼‰
    def _top5(c):
        items = by_code.get(c, [])[:5]
        return [{"title": t, "url": u} for (_day_tag, t, u) in items]

    df_out["_é–‹ç¤ºä¸Šä½5"] = df_out["code"].apply(_top5)

    return df_out


# ========= Kabutan PTS =========

PTS_URL_TEMPLATE = "https://s.kabutan.jp/warnings/pts_night_price_increase/?page={page}"


def fetch_pts_page(page: int) -> str:
    url = PTS_URL_TEMPLATE.format(page=page)
    r = requests.get(url, timeout=20)
    r.raise_for_status()
    return r.text


def parse_pts_page(html: str) -> pd.DataFrame:
    soup = BeautifulSoup(html, "html.parser")
    table = soup.select_one("div.gray-sticky-table table")
    if table is None:
        return pd.DataFrame()

    tbody = table.find("tbody")
    if tbody is None:
        return pd.DataFrame()

    rows = []
    for tr in tbody.find_all("tr"):
        th = tr.find("th")
        tds = tr.find_all("td")
        if th is None or len(tds) < 4:
            continue

        th_text = th.get_text(" ", strip=True)
        m = re.search(r"(\d{4})", th_text)
        if m is None:
            continue
        code = m.group(1)
        name = th_text.replace(code, "").strip()

        close_price = _to_int(tds[0].get_text(strip=True))
        pts_price = _to_int(tds[1].get_text(strip=True))

        pct_raw = tds[2].get_text(strip=True)
        pct = _to_float_pct(pct_raw)

        volume = _to_int(tds[3].get_text(strip=True))

        rows.append(
            {
                "code": code,
                "name": name,
                "pct": pct,
                "volume": volume,
                "close_price": close_price,
                "pts_price": pts_price,
                "pct_raw": pct_raw,
            }
        )

    return pd.DataFrame(rows)


def crawl_until_below_threshold(pct_threshold: float, max_pages: int, debug: bool = False) -> Tuple[pd.DataFrame, int]:
    all_df = []
    last_page = 0

    for page in range(1, max_pages + 1):
        html = fetch_pts_page(page)
        df = parse_pts_page(html)
        last_page = page

        if df is None or len(df) == 0:
            if debug:
                st.write(f"ã€è¨ºæ–­ã€‘Kabutan page={page}: 0ä»¶ï¼ˆåœæ­¢ï¼‰")
            break

        mx = None
        if "pct" in df.columns:
            mx0 = df["pct"].max(skipna=True)
            if mx0 is not None and not pd.isna(mx0):
                try:
                    mx = float(mx0)
                except Exception:
                    mx = None

        all_df.append(df)

        # â˜…ä¿®æ­£ï¼šmxãŒNaN/Noneãªã‚‰å®‰å…¨å´ã§åœæ­¢
        if debug:
            st.write(f"ã€è¨ºæ–­ã€‘Kabutan page={page}: max pct =", mx if mx is not None else "None/NaN")
        if mx is None or mx < pct_threshold:
            break

    if len(all_df) == 0:
        return pd.DataFrame(), last_page

    out = pd.concat(all_df, ignore_index=True)
    return out, last_page


# ========= UI =========

# æ¨ªå¹…æœ€å¤§åŒ–ï¼ˆã“ã‚ŒãŒåŠ¹ãï¼‰
st.set_page_config(layout="wide")

st.title("PTSãƒŠã‚¤ãƒˆã‚¿ã‚¤ãƒ ä¸Šæ˜‡ç‡ãƒ©ãƒ³ã‚­ãƒ³ã‚° + TDneté©æ™‚é–‹ç¤º")

debug = st.checkbox("è¨ºæ–­è¡¨ç¤ºï¼ˆé–‹ç™ºç”¨ï¼‰", value=False)

# â˜…è¿½åŠ ï¼šå‡¡ä¾‹
st.caption("ğŸŸ¦ï¼å½“æ—¥ã€€ğŸŸ¨ï¼å‰æ—¥")

pct_min = st.text_input("ä¸Šæ˜‡ç‡(%)ã®ä¸‹é™", value="5")
vol_min = st.text_input("å‡ºæ¥é«˜ã®ä¸‹é™", value="1000")
max_pages = st.text_input("æœ€å¤§ãƒšãƒ¼ã‚¸æ•°ï¼ˆå®‰å…¨ã®ãŸã‚ï¼‰", value="30")

if st.button("å–å¾—ã—ã¦è¡¨ç¤º"):
    try:
        pct_min_val = _to_float_pct(pct_min)
        vol_min_val = _to_int(vol_min)
        max_pages_val = _to_int(max_pages)

        if pct_min_val is None:
            raise ValueError(f"ä¸Šæ˜‡ç‡(%)ã®ä¸‹é™ãŒè§£é‡ˆã§ãã¾ã›ã‚“: {pct_min}")
        if vol_min_val is None:
            raise ValueError(f"å‡ºæ¥é«˜ã®ä¸‹é™ãŒè§£é‡ˆã§ãã¾ã›ã‚“: {vol_min}")
        if max_pages_val is None or max_pages_val <= 0:
            raise ValueError(f"æœ€å¤§ãƒšãƒ¼ã‚¸æ•°ãŒè§£é‡ˆã§ãã¾ã›ã‚“: {max_pages}")

        df, last_page = crawl_until_below_threshold(
            pct_threshold=float(pct_min_val),
            max_pages=int(max_pages_val),
            debug=debug,
        )

        df2 = df.dropna(subset=["pct", "volume"]).copy()
        df2 = df2[(df2["pct"] >= float(pct_min_val)) & (df2["volume"] >= int(vol_min_val))].copy()

        df2 = attach_disclosures(df2, debug=debug)

        hit = df2[df2["é–‹ç¤ºä»¶æ•°"] > 0].copy()
        st.write(f"ã€é›†è¨ˆã€‘é–‹ç¤ºã‚ã‚Š: {len(hit)} / é–‹ç¤ºãªã—: {len(df2) - len(hit)}")
        st.success(
            f"{last_page}ãƒšãƒ¼ã‚¸ç›®ã¾ã§å·¡å›ã€‚æŠ½å‡º {len(df2)} ä»¶ï¼ˆpct>={pct_min_val}, volume>={vol_min_val}ï¼‰"
        )

        # è¡¨ãŒå¤ªããªã‚‰ãªã„ã‚ˆã†çŸ­ãæ•´å½¢
        df_show = df2.copy()
        df_show["pct"] = df_show["pct"].apply(lambda x: "" if pd.isna(x) else f"{float(x):.2f}")
        df_show["volume"] = df_show["volume"].apply(lambda x: "" if pd.isna(x) else f"{int(x):,}")
        df_show["é–‹ç¤ºä»¶æ•°"] = df_show["é–‹ç¤ºä»¶æ•°"].apply(lambda x: "" if pd.isna(x) else str(int(x)))

        cols = [
            "code", "name", "pct", "volume",
            "é–‹ç¤ºä»¶æ•°",
            "é–‹ç¤ºã‚¿ã‚¤ãƒˆãƒ«1", "PDFãƒªãƒ³ã‚¯1",
            "é–‹ç¤ºã‚¿ã‚¤ãƒˆãƒ«2", "PDFãƒªãƒ³ã‚¯2",
            "é–‹ç¤ºã‚¿ã‚¤ãƒˆãƒ«3", "PDFãƒªãƒ³ã‚¯3",
        ]
        df_show = df_show.reindex(columns=cols)

        def _linkcol(colname: str):
            try:
                return st.column_config.LinkColumn(colname, display_text="PDF")
            except TypeError:
                return st.column_config.LinkColumn(colname)

        st.dataframe(
            df_show,
            use_container_width=True,
            hide_index=True,
            column_config={
                "PDFãƒªãƒ³ã‚¯1": _linkcol("PDFãƒªãƒ³ã‚¯1"),
                "PDFãƒªãƒ³ã‚¯2": _linkcol("PDFãƒªãƒ³ã‚¯2"),
                "PDFãƒªãƒ³ã‚¯3": _linkcol("PDFãƒªãƒ³ã‚¯3"),
            },
        )

        # è©³ç´°ï¼ˆæœ€å¤§5ä»¶ï¼‰
        for _, row in hit.iterrows():
            code = row.get("code", "")
            name = row.get("name", "")
            items = row.get("_é–‹ç¤ºä¸Šä½5", [])
            total = int(row.get("é–‹ç¤ºä»¶æ•°", 0))

            note = ""
            if total > 5:
                note = f"ï¼ˆå…¨{total}ä»¶ã®ã†ã¡ä¸Šä½5ä»¶ã®ã¿è¡¨ç¤ºï¼‰"

            with st.expander(f"{code} {name} ã®é©æ™‚é–‹ç¤º {note}"):
                for it in items:
                    t = _safe_text(it.get("title", ""))
                    u = _safe_text(it.get("url", ""))
                    if u:
                        st.markdown(f"- [{t}]({u})")
                    else:
                        st.markdown(f"- {t}")

    except Exception as e:
        st.error(f"å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
else:
    st.info("æ¡ä»¶ã‚’è¨­å®šã—ã¦ã€Œå–å¾—ã—ã¦è¡¨ç¤ºã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")


        

